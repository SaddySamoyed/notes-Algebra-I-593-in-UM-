\documentclass[lang=cn,11pt]{template}
\usepackage[utf8]{inputenc}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}%
\usepackage{amssymb}%

\title{593}

\begin{document}
\frontmatter
\tableofcontents
\mainmatter

\chapter{Rings}

\begin{definition}
A ring is a set \( R \) with two operations:
\begin{itemize}
    \item \( + : R \times R \to R \) (called addition),
    \item \( * : R \times R \to R \) (called multiplication),
\end{itemize}
and elements \( 0_R \) and \( 1_R \) satisfying the following axioms:
\begin{itemize}
    \item[R1:] \( (R, +, 0_R) \) is an abelian group.
    \item[R2:] \( * \) is associative: for all \( r, s, t \in R \), \( r * (s * t) = (r * s) * t \).
    \item[R3:] Multiplication is both left and right distributive with respect to addition: for all \( r, s, t \in R \),
    \[
    r * (s + t) = r * s + r * t \quad \text{and} \quad (s + t) * r = s * r + t * r.
    \]
    \item[R4:] \( 1_R * r = r * 1_R = r \) for all \( r \in R \).
\end{itemize}
We often drop the symbol \( * \) and write \( ab \) for \( a * b \), and write \( 0 \) and \( 1 \) for \( 0_R \) and \( 1_R \). A ring is said to be \textit{commutative} if its multiplication is commutative. A \textit{zero ring} is a ring with one element.
\end{definition}

\section{Properties of Rings}

\subsection*{P1.1} Suppose \( R \) is a ring. Show that \( \text{Mat}_{n \times n}(R) \) is a ring with respect to matrix multiplication.

\subsection*{P1.2} Let \( G \) be a group and \( k \) a ring. The \textit{group ring} \( kG \) is defined to be the set of sums of the form
\[
\sum_{g \in G} a_g g,
\]
where \( a_g \in k \) and only finitely many \( a_g \) are nonzero. Define addition and multiplication for this ring, and show it satisfies the ring axioms.

\subsection*{P1.3} Let \( A \) be an abelian group, and define \( R = \text{Hom}_{\text{grp}}(A, A) \), with operations \( + \) and \( * \) defined by
\[
(r_1 + r_2)(a) = r_1(a) + r_2(a) \quad \text{and} \quad (r_1 * r_2)(a) = r_1(r_2(a)).
\]
Show that \( R \) forms a ring, called the \textit{endomorphism ring} of \( A \), denoted \( \text{End}(A) \).

\subsection*{P1.4} Explain why it is necessary that \( A \) is abelian in the previous problem.

\subsection*{P1.5} Suppose \( R \) is a ring. Show that \( 0_R * x = x * 0_R = 0_R \) for all \( x \in R \).

\subsection*{P1.6} Suppose \( R \) is a ring with \( 0_R = 1_R \). Show that \( R \) must be the zero ring.

\begin{definition}
Suppose \( R \) is a ring. An element \( u \in R \) is called a \textit{unit} if there exists \( u^{-1} \in R \) such that \( u * u^{-1} = u^{-1} * u = 1_R \). The set of all units in \( R \) is denoted \( R^\times \).
\end{definition}

\subsection*{P1.7} Show that \( R^\times \) forms a group under the multiplication \( * \) of \( R \).

\begin{definition}
Suppose \( (R, +_R, *_R, 1_R) \) and \( (S, +_S, *_S, 1_S) \) are rings. A function \( f : R \to S \) is called a \textit{ring homomorphism} if:
\begin{itemize}
    \item \( f(a +_R b) = f(a) +_S f(b) \) for all \( a, b \in R \),
    \item \( f(a *_R b) = f(a) *_S f(b) \) for all \( a, b \in R \),
    \item \( f(1_R) = 1_S \).
\end{itemize}
The set of ring homomorphisms from \( R \) to \( S \) is denoted \( \text{Hom}(R, S) \) or \( \text{Hom}_{\text{ring}}(R, S) \).
\end{definition}

\subsection*{P1.8} Let \( R = \mathbb{Z}/15\mathbb{Z} \) and \( S = \mathbb{Z}/3\mathbb{Z} \). Determine \( \text{Hom}_{\text{ring}}(R, S) \) and \( \text{Hom}_{\text{ring}}(S, R) \), including the case of non-unital homomorphisms.

\subsection*{P1.9} We defined a group ring above. For those who know what a monoid and/or a category are: Can you define a monoid ring? What about a category ring?











\chapter{Modules}

\begin{definition}
Suppose \( R \) is a ring. A left \( R \)-module is a set \( M \) with two operations:
\begin{itemize}
    \item \( + : M \times M \to M \) (called addition),
    \item \( * : R \times M \to M \) (called scalar multiplication),
\end{itemize}
and an element \( 0_M \) satisfying the following axioms:
\begin{itemize}
    \item[M1:] \( (M, +, 0_M) \) is an abelian group.
    \item[M2:] \( (r + s) * m = r * m + s * m \) for all \( r, s \in R \) and \( m \in M \).
    \item[M3:] \( (rs) * m = r * (s * m) \) for all \( r, s \in R \) and \( m \in M \).
    \item[M4:] \( r * (m + n) = r * m + r * n \) for all \( r \in R \) and \( m, n \in M \).
    \item[M5:] \( 1_R * m = m \) for all \( m \in M \).
\end{itemize}
An \( R \)-module \( M \) with this structure is also called a left \( R \)-module. The map \( * : R \times M \to M \) is called an \textit{action} of \( R \) on \( M \), and elements of \( R \) are often referred to as \textit{scalars}.
\end{definition}

\section{Properties of Modules}

\subsection*{P2.1} Show that \( \mathbb{Z}^n \) is a left \( \text{Mat}_{n \times n}(\mathbb{Z}) \)-module by defining the action of an element \( X \in \text{Mat}_{n \times n}(\mathbb{Z}) \) on \( v \in \mathbb{Z}^n \) as the matrix-vector product \( Xv \).

\begin{definition}
Suppose \( R \) is a ring, and \( M \) and \( N \) are \( R \)-modules. A function \( g : M \to N \) is called an \textit{R-module homomorphism} if:
\begin{itemize}
    \item \( g \) is a group homomorphism,
    \item \( g(rm) = rg(m) \) for all \( r \in R \) and \( m \in M \).
\end{itemize}
The set of \( R \)-module homomorphisms from \( M \) to \( N \) is denoted \( \text{Hom}_R(M, N) \). We define \( \text{End}_R(M) := \text{Hom}_R(M, M) \), called the \textit{endomorphism ring} of \( M \).
\end{definition}

\subsection*{P2.2} Suppose \( R \) is a commutative ring and \( M \) is an \( R \)-module. Show that there is a “natural” map of rings \( R \to \text{End}_R(M) \). What changes if \( R \) is not commutative?

\begin{definition}
Suppose \( R \) is a ring, and \( M \) and \( N \) are \( R \)-modules. The \textit{direct sum} of \( M \) and \( N \), denoted \( M \oplus N \), is the \( R \)-module defined as follows: An element of \( M \oplus N \) is an ordered pair \( (m, n) \) where \( m \in M \) and \( n \in N \). The operations are defined by
\[
(m_1, n_1) + (m_2, n_2) = (m_1 + m_2, n_1 + n_2) \quad \text{and} \quad r(m, n) = (rm, rn).
\]
\end{definition}

\subsection*{P2.3} Verify that \( M \oplus N \) is an \( R \)-module.

\subsection*{P2.4} Let \( M_1, M_2, M, N_1, N_2, \) and \( N \) be \( R \)-modules. Show that \( \text{Hom}_R(M_1 \oplus M_2, N) \cong \text{Hom}_R(M_1, N) \times \text{Hom}_R(M_2, N) \) and \( \text{Hom}_R(M, N_1 \oplus N_2) \cong \text{Hom}_R(M, N_1) \times \text{Hom}_R(M, N_2) \) as abelian groups.

\subsection*{P2.5} Let \( L_1, L_2, \dots, L_p, M_1, M_2, \dots, M_q, N_1, N_2, \dots, N_r \) be \( R \)-modules, and let \( L = \bigoplus L_i \), \( M = \bigoplus M_j \), and \( N = \bigoplus N_k \). Describe a way to represent elements of \( \text{Hom}_R(L, M) \), \( \text{Hom}_R(M, N) \), and \( \text{Hom}_R(L, N) \) as matrices so that the composition map \( \text{Hom}_R(L, M) \times \text{Hom}_R(M, N) \to \text{Hom}_R(L, N) \) corresponds to matrix multiplication.













\chapter{Ideals}

\begin{definition}
Suppose \( R \) is a ring. A subset \( I \subset R \) is called a \textit{left ideal} if it satisfies:
\begin{itemize}
    \item[I1:] \( (I, +) \) is a subgroup of \( (R, +) \),
    \item[I2:] For all \( r \in R \), \( rI \subset I \), meaning \( rx \in I \) for all \( x \in I \).
\end{itemize}
Similarly, \( I \) is a \textit{right ideal} if:
\begin{itemize}
    \item[I1:] \( (I, +) \) is a subgroup of \( (R, +) \),
    \item[I2:] For all \( r \in R \), \( Ir \subset I \), meaning \( yr \in I \) for all \( y \in I \).
\end{itemize}
A subset of \( R \) that is both a left and right ideal is called a \textit{two-sided ideal}. If \( R \) is commutative, left, right, and two-sided ideals coincide, so we simply refer to such subsets as \textit{ideals}.
\end{definition}

\section{Basic Properties of Ideals}

\subsection*{P3.1} Show that if \( A \) and \( B \) are ideals, then \( A + B := \{ a + b : a \in A, b \in B \} \) is also an ideal.

\subsection*{P3.2} Let \( n \geq 2 \). Let \( I \) be the subset of \( R = \text{Mat}_{n \times n}(\mathbb{Q}) \) consisting of matrices with nonzero entries only in the first row. Determine whether \( I \) is a left ideal, a right ideal, or both.

\subsection*{P3.3} Suppose \( R \) and \( S \) are rings and \( \varphi \in \text{Hom}(R, S) \). Show that \( \ker(\varphi) \) is a two-sided ideal of \( R \).

\section{Quotient Structures and Ideals}

\subsection*{P3.4} Let \( R \) be a ring and \( I \) a left ideal. Since \( I \) and \( R \) are abelian groups with respect to \( + \), we can form the quotient group \( R/I \). Show that \( R/I \) has a natural structure as a left \( R \)-module.

\subsection*{P3.5} Let \( R \) be a ring and \( I \) a two-sided ideal. Show that \( R/I \) has a natural ring structure.









\chapter{Integral Domains}

\begin{definition}
A commutative ring \( R \) is called an \textit{integral domain} if:
\begin{itemize}
    \item[ID1:] Whenever \( xy = 0 \) in \( R \), we have either \( x = 0 \) or \( y = 0 \).
    \item[ID2:] The ring \( R \) is not the zero ring.
\end{itemize}
\end{definition}

\section{Basic Properties of Integral Domains}

\subsection*{P4.1} Show that a field is an integral domain.

\subsection*{P4.2} Show that \( \mathbb{Z} \) is an integral domain but not a field.

\subsection*{P4.3} Show that \( k[x] \) is an integral domain but not a field, where \( k \) is a field.

\subsection*{P4.4} Let \( R \) be a nonzero commutative ring.
\begin{enumerate}
    \item Show that \( R \) is an integral domain if and only if, for all \( x \neq 0 \) in \( R \), the map \( y \mapsto xy \) is injective.
    \item Show that \( R \) is a field if and only if, for all \( x \neq 0 \) in \( R \), the map \( y \mapsto xy \) is bijective.
\end{enumerate}

\section{Finite Integral Domains and Fields}

\subsection*{P4.5} Let \( R \) be an integral domain and suppose that \( \#(R) \) is finite. Show that \( R \) is a field.

\subsection*{P4.6} Let \( R \) be an integral domain, and let \( k \) be a subring of \( R \) which is a field, such that \( R \) is finite-dimensional as a \( k \)-vector space. Show that \( R \) is a field.

\begin{remark}
Every integral domain \( R \) embeds in a natural field, known as the field of fractions of \( R \) and denoted \( \text{Frac}(R) \).
\end{remark}

\begin{definition}
Let \( R \) be an integral domain. Define \( X \) to be the set of pairs \( (p, q) \) in \( R^2 \) with \( q \neq 0 \). Define an equivalence relation \( \sim \) on \( X \) by
\[
(p_1, q_1) \sim (p_2, q_2) \quad \text{if and only if} \quad p_1 q_2 = p_2 q_1.
\]
We denote an element of \( X / \sim \) as \( \frac{p}{q} \) or \( pq^{-1} \). We define addition and multiplication on \( X / \sim \) by:
\[
\frac{p_1}{q_1} + \frac{p_2}{q_2} = \frac{p_1 q_2 + p_2 q_1}{q_1 q_2} \quad \text{and} \quad \frac{p_1}{q_1} * \frac{p_2}{q_2} = \frac{p_1 p_2}{q_1 q_2}.
\]
This field is denoted \( \text{Frac}(R) \).
\end{definition}

\section{Properties of the Field of Fractions}

\subsection*{P4.7} Verify that \( \sim \) is an equivalence relation on \( X \).

\subsection*{P4.8} Verify that \( X / \sim \) is a field under the operations \( + \) and \( * \) defined above.

\begin{remark}
This construction of \( \text{Frac}(R) \) illustrates why it is essential that \( \{0\} \) is not defined as an integral domain. If \( R = \{0\} \), then \( X = \emptyset \), making \( \text{Frac}(R) \) empty and lacking both additive and multiplicative identities.
\end{remark}







\chapter{Prime and Maximal Ideals}

\begin{definition}
Suppose \( R \) is a commutative ring. An ideal \( P \) of \( R \) is called \textit{prime} if:
\begin{itemize}
    \item[P1:] For all \( a, b \in R \), if \( ab \in P \), then \( a \in P \) or \( b \in P \).
    \item[P2:] The ideal \( P \) is not equal to all of \( R \).
\end{itemize}
\end{definition}

\section{Properties of Prime Ideals}

\subsection*{P5.1} Let \( R \) be a commutative ring, and let \( I \) be an ideal of \( R \). Show that \( I \) is prime if and only if \( R/I \) is an integral domain.

\subsection*{P5.2} For which integers \( n \) is \( n\mathbb{Z} \) a prime ideal? Assume the uniqueness of prime factorization for this question.

\begin{definition}
Suppose \( R \) is a commutative ring. An ideal \( m \) of \( R \) is called \textit{maximal} if:
\begin{itemize}
    \item[M1:] For all \( a \in R \), if \( a \notin m \), then there exists \( b \in R \) such that \( ab \equiv 1 \pmod{m} \).
    \item[M2:] The ideal \( m \) is not equal to all of \( R \).
\end{itemize}
\end{definition}

\section{Properties of Maximal Ideals}

\subsection*{P5.3} Let \( R \) be a commutative ring, and let \( I \) be an ideal of \( R \). Show that \( I \) is maximal if and only if \( R/I \) is a field.

\subsection*{P5.4} Show that every maximal ideal is a prime ideal.

\subsection*{P5.5} Show that an ideal \( I \subset R \) is maximal if and only if there does not exist an ideal \( J \) such that \( I \subset J \subset R \) with \( I \neq J \) and \( J \neq R \).

\begin{remark}
Problem P5.5 provides the motivation for the term "maximal." Using Zorn’s lemma and Problem P5.5, it can be shown that every ideal in a nonzero commutative ring is contained in a maximal ideal.
\end{remark}

\section{Examples of Prime and Maximal Ideals}

\subsection*{P5.6} Let \( R = \mathbb{R}[x, y] \). Show that \( yR \) is a prime ideal but not a maximal ideal.

\subsection*{P5.7} Let \( R \) be a commutative ring, and let \( P \) be a prime ideal. If \( R/P \) is finite, show that \( P \) is a maximal ideal.

\subsection*{P5.8} Determine the maximal ideals of \( \mathbb{Z} \).



\chapter{Products of Rings and Modules}

\begin{definition}
Recall that if \( A \) and \( B \) are sets, then the product of \( A \) and \( B \) is the set \( A \times B = \{ (a, b) \mid a \in A, b \in B \} \). This concept can be extended to a product of any number of sets. If \( R \) and \( S \) are rings, we want the product \( R \times S \) to have a ring structure. To achieve this, we define addition and multiplication as follows:
\begin{itemize}
    \item \( (r, s) + (r', s') = (r + r', s + s') \) for all \( (r, s), (r', s') \in R \times S \),
    \item \( (r, s) * (r', s') = (r * r', s * s') \) for all \( (r, s), (r', s') \in R \times S \).
\end{itemize}
\end{definition}

\section{Isomorphisms and Homomorphisms of Ring Products}

\subsection*{P6.1} Show that \( \mathbb{Z}/3\mathbb{Z} \times \mathbb{Z}/5\mathbb{Z} \) and \( \mathbb{Z}/15\mathbb{Z} \) are isomorphic as rings.

\subsection*{P6.2} Are there natural ring homomorphisms \( R \to R \times S \) and \( S \to R \times S \)? Are there natural ring homomorphisms \( R \times S \to R \) and \( R \times S \to S \)?

\section{Module Structures on Product Rings}

\subsection*{P6.3} Let \( R \) and \( S \) be rings, and let \( M \) and \( N \) be an \( R \)-module and an \( S \)-module, respectively. Describe how to define an \( (R \times S) \)-module structure on the abelian group \( M \times N \).

\subsection*{P6.4} Let \( R \) and \( S \) be rings. Define \( e \) as the element \( (1, 0) \in R \times S \). Let \( M \) be an \( R \times S \)-module.
\begin{enumerate}
    \item Show that \( M = eM \oplus (1 - e)M \).
    \item Describe how to equip \( eM \) with the structure of an \( R \)-module, and \( (1 - e)M \) with the structure of an \( S \)-module, such that \( M \cong eM \times (1 - e)M \) (as in Problem P6.3).
\end{enumerate}











\chapter{Comaximal Ideals}

\begin{definition}
Suppose \( R \) is a commutative ring. Ideals \( A \) and \( B \) of \( R \) are said to be \textit{comaximal} if \( A + B = R \).
\end{definition}

\section{Basic Properties of Comaximal Ideals}

\subsection*{P7.1} Show that \( A \) and \( B \) are comaximal if and only if \( 1 \in A + B \).

\subsection*{P7.2} If \( m \) is a maximal ideal and \( I \) is an ideal of \( R \), show that either \( m \) and \( I \) are comaximal, or \( I \subseteq m \).

\subsection*{P7.3} Let \( R \) be a commutative ring and let \( A \) and \( B \) be ideals. Show that the map \( R \to R/A \times R/B \), defined by sending \( r \) to the ordered pair \( (r \mod A, r \mod B) \), is surjective if and only if \( A \) and \( B \) are comaximal.

\begin{definition}
Suppose \( R \) is a ring. The \textit{product} of ideals \( A \) and \( B \) in \( R \) is the ideal, denoted \( AB \), consisting of all finite sums \( \sum a_i b_i \) with \( (a_i, b_i) \in A \times B \). The product of any finite number of ideals is defined similarly.
\end{definition}

\section{Further Properties of Comaximal Ideals}

\subsection*{P7.4} Suppose \( A \) and \( B \) are comaximal ideals in a commutative ring \( R \). Show that \( A \cap B = AB \).

\subsection*{P7.5} Suppose \( R \) is a nonzero commutative ring. Let \( I_1, I_2, I_3, \ldots, I_k \) be ideals in \( R \) that are pairwise comaximal. Show that \( I_1 \) and \( I_2 I_3 \cdots I_k \) are comaximal.

\section{Comaximal Ideals vs. Relatively Prime Elements}

\subsection*{P7.6} Let \( R \) be a commutative ring, and let \( a \) and \( b \) be elements in \( R \). Suppose that \( aR \) and \( bR \) are comaximal. Show that any \( g \) which divides both \( a \) and \( b \) must be a unit.

\subsection*{P7.7} Show that the ideals \( xk[x, y] \) and \( yk[x, y] \) are not comaximal, although the polynomials \( x \) and \( y \) are relatively prime in \( k[x, y] \).

\subsection*{P7.8} Let \( a \) and \( b \) be relatively prime integers. Show that the ideals \( a\mathbb{Z} \) and \( b\mathbb{Z} \) are comaximal.





\chapter{The Chinese Remainder Theorem}

\begin{remark}
“There are certain things whose number is unknown. If we count them by threes, we have two left over; by fives, we have three left over; and by sevens, two are left over. How many things are there?” — Sunzi Suanjing (3rd century)
\end{remark}

\section{Comaximal Ideals and Ring Homomorphisms}

\subsection*{P8.1} Let \( R \) be a commutative ring, and let \( A \) and \( B \) be ideals. Describe the “obvious” map \( R \to R/A \times R/B \) and show that its kernel is \( A \cap B \).

\subsection*{P8.2} Show that if \( R \) is a commutative ring and \( A \) and \( B \) are comaximal ideals, then \( R/AB \cong R/A \times R/B \).

\section{The Chinese Remainder Theorem}

\subsection*{P8.3} (\textbf{The Chinese Remainder Theorem}) Show that if \( I_1, I_2, \ldots, I_k \) are pairwise comaximal ideals, then
\[
R/(I_1 I_2 \cdots I_k) \cong R/I_1 \times R/I_2 \times \cdots \times R/I_k.
\]

\subsection*{P8.4} Show that if \( m_1, m_2, \ldots, m_k \) are pairwise relatively prime integers, then
\[
\mathbb{Z}/(m_1 \cdots m_k)\mathbb{Z} \cong \mathbb{Z}/m_1\mathbb{Z} \times \cdots \times \mathbb{Z}/m_k\mathbb{Z}.
\]

\subsection*{P8.5} Let \( k \) be a field and let \( a_1, a_2, \ldots, a_r \) be distinct elements of \( k \). Show that
\[
k[t]/((t - a_1)(t - a_2) \cdots (t - a_r)k[t]) \cong k \times \cdots \times k,
\]
where the right-hand side has \( r \) factors.










\chapter{Simple Modules}

\begin{definition}
Let \( R \) be a ring, and let \( S \) be a (left) \( R \)-module. The module \( S \) is called \textit{simple} if \( S \neq 0 \) and the only \( R \)-submodules of \( S \) are \( (0) \) and \( S \).
\end{definition}

\section{Properties of Simple Modules}

\subsection*{P9.1} Let \( R \) be a ring, let \( S \) be a simple \( R \)-module, and let \( M \) be any \( R \)-module.
\begin{enumerate}
    \item Let \( \alpha : S \to M \) be an \( R \)-module homomorphism. Show that \( \alpha \) is either injective or zero.
    \item Let \( \beta : M \to S \) be an \( R \)-module homomorphism. Show that \( \beta \) is either surjective or zero.
\end{enumerate}

\subsection*{P9.2} Let \( R \) be a ring and \( I \) a left ideal of \( R \). Show that \( R/I \) is simple if and only if there are no left ideals \( J \) such that \( I \subset J \subset R \). Such a left ideal is called a \textit{maximal left ideal}.

\subsection*{P9.3} If \( R \) is a commutative ring, show that this notion of “maximal left ideal” coincides with the notion of “maximal ideal” defined previously.

\section{Generation of Simple Modules}

\subsection*{P9.4} If \( S \) is a simple module and \( x \) is any nonzero element of \( S \), show that \( S = Rx \).

\subsection*{P9.5} In any module \( M \), if there is an element \( x \) such that \( M = Rx \), show that there is a left ideal \( I \) of \( R \) such that \( M \cong R/I \).

\begin{remark}
We have shown that the simple \( R \)-modules are precisely the \( R \)-modules of the form \( R/I \) for \( I \) a maximal left ideal.
\end{remark}

\section{Schur's Lemma}

\begin{theorem}[Schur's Lemma]
Let \( M \) be a simple \( R \)-module. Let \( \varphi : M \to M \) be an \( R \)-module homomorphism. Show that either \( \varphi = 0 \) or \( \varphi \) is invertible.
\end{theorem}

\begin{remark}
Schur’s Lemma is the first of many results relating a property of a module to a property of its endomorphism ring.
\end{remark}










\chapter{Composition Series}

\begin{definition}
Let \( R \) be a ring, and let \( M \) be an \( R \)-module. A \textit{chain of submodules} of \( M \) is a sequence \( 0 = M_0 \subseteq M_1 \subseteq \cdots \subseteq M_\ell = M \). The integer \( \ell \) is called the \textit{length} of the chain.
\end{definition}

\begin{definition}
A \textit{composition series} is a chain of submodules \( 0 = M_0 \subset M_1 \subset \cdots \subset M_\ell = M \) such that each quotient module \( M_i / M_{i-1} \) is simple. Note that the zero module is not considered simple, so \( M_i \neq M_{i+1} \) in a composition series.
\end{definition}

\section{Existence and Finite Length of Composition Series}

\subsection*{P10.1} Suppose there is a positive integer \( L \) such that, for any chain \( 0 = M_0 \subset M_1 \subset \cdots \subset M_\ell = M \), we have \( \ell \leq L \). Show that \( M \) has a composition series. (Hint: Consider a chain of maximal length.)

\begin{definition}
We say that \( M \) has \textit{finite length} if \( M \) has a composition series.
\end{definition}

\subsection*{P10.2} Let \( M \) be an \( R \)-module that is a finite set. Show that \( M \) has finite length.

\subsection*{P10.3} Let \( k \) be a field contained in \( R \). Suppose that \( M \) is finite-dimensional as a \( k \)-vector space. Show that \( M \) has finite length.

\section{Quasi-Composition Series}

\begin{definition}
A \textit{quasi-composition series} is a chain of submodules \( 0 = M_0 \subseteq M_1 \subseteq \cdots \subseteq M_\ell = M \) such that each quotient module \( M_i / M_{i-1} \) is either simple or zero.
\end{definition}

\subsection*{P10.4} Show that, if \( M \) has a quasi-composition series, then \( M \) has a composition series.

\subsection*{P10.5} Let \( \alpha : A \hookrightarrow B \) be an injective \( R \)-module homomorphism, and let \( 0 = B_0 \subset B_1 \subset \cdots \subset B_b = B \) be a composition series. Show that \( \alpha^{-1}(B_0) \subseteq \alpha^{-1}(B_1) \subseteq \cdots \subseteq \alpha^{-1}(B_b) \) is a quasi-composition series.

\subsection*{P10.6} Let \( \beta : B \twoheadrightarrow C \) be a surjective \( R \)-module homomorphism, and let \( 0 = B_0 \subset B_1 \subset \cdots \subset B_b = B \) be a composition series. Show that \( \beta(B_0) \subset \beta(B_1) \subset \cdots \subset \beta(B_b) \) is a quasi-composition series.

\begin{remark}
The property of having a composition series passes to submodules and to quotient modules.
\end{remark}






\chapter{The Jordan-Hölder Theorem}

\begin{definition}
A \textit{short exact sequence} of \( R \)-modules is a sequence of three \( R \)-modules \( A \), \( B \), and \( C \), along with two \( R \)-module homomorphisms \( \alpha : A \to B \) and \( \beta : B \to C \) such that \( \alpha \) is injective, \( \beta \) is surjective, and \( \text{Im}(\alpha) = \ker(\beta) \). We write it as
\[
0 \to A \xrightarrow{\alpha} B \xrightarrow{\beta} C \to 0.
\]
\end{definition}

Throughout this worksheet, let \( R \) be a ring, and let \( 0 \to A \to B \to C \to 0 \) be a short exact sequence of \( R \)-modules. Previously, we noted that if \( B_0 \subset B_1 \subset \cdots \subset B_\ell \) is a composition series for \( B \), then \( \alpha^{-1}(B_0) \subseteq \alpha^{-1}(B_1) \subseteq \cdots \subseteq \alpha^{-1}(B_\ell) \) is a quasi-composition series for \( A \), and \( \beta(B_0) \subseteq \beta(B_1) \subseteq \cdots \subseteq \beta(B_\ell) \) is a quasi-composition series for \( C \).

\section{Properties of Short Exact Sequences}

\subsection*{P11.1} With the notation above, show that exactly one of the following holds:
\begin{enumerate}
    \item \( \alpha^{-1}(B_{i-1}) = \alpha^{-1}(B_i) \) and \( \beta(B_i) / \beta(B_{i-1}) \cong B_i / B_{i-1} \),
    \item \( \alpha^{-1}(B_i) / \alpha^{-1}(B_{i-1}) \cong B_i / B_{i-1} \) and \( \beta(B_{i-1}) = \beta(B_i) \).
\end{enumerate}

\section{Jordan-Hölder Theorem}

\begin{definition}
Let \( M \) be an \( R \)-module of finite length, and let \( 0 = M_0 \subset M_1 \subset \cdots \subset M_m = M \) be a composition series. We define \( \ell(M, M_\bullet) \) to be the length \( m \) of the composition series \( M_\bullet \). For any simple module \( S \), we define \( \text{Mult}(S, M, M_\bullet) \) as the number of indices \( i \) for which \( M_i / M_{i-1} \cong S \).
\end{definition}

\begin{theorem}[Jordan-Hölder]
Let \( M \) be an \( R \)-module of finite length. Suppose that \( M \) has two composition series, \( 0 = M_0 \subset M_1 \subset \cdots \subset M_m = M \) and \( 0 = M'_0 \subset M'_1 \subset \cdots \subset M'_n = M \). Then \( \ell(M, M_\bullet) = \ell(M, M'_\bullet) \), and for any simple module \( S \), we have \( \text{Mult}(S, M, M_\bullet) = \text{Mult}(S, M, M'_\bullet) \).
\end{theorem}

\begin{remark}
The Jordan-Hölder theorem shows that \( \ell(M) \) and \( \text{Mult}(S, M) \) are well-defined quantities.
\end{remark}

\section{Applications of the Jordan-Hölder Theorem}

\subsection*{P11.2} Let \( B_\bullet \) be a composition series for \( B \). Define \( \tilde{A}_i = \alpha^{-1}(B_i) \) and \( \tilde{C}_i = \beta(B_i) \), and let \( A_\bullet \) and \( C_\bullet \) be the composition series obtained from deleting duplicate elements from \( \tilde{A}_\bullet \) and \( \tilde{C}_\bullet \). Show that \( \ell(B, B_\bullet) = \ell(A, A_\bullet) + \ell(C, C_\bullet) \), and that for any simple module \( S \), we have \( \text{Mult}(S, B, B_\bullet) = \text{Mult}(S, A, A_\bullet) + \text{Mult}(S, C, C_\bullet) \).

\subsection*{P11.3} Show that if the Jordan-Hölder theorem holds for \( A \) and \( C \), then it holds for \( B \).

\subsection*{P11.4} Show that the Jordan-Hölder theorem holds if the module \( M \) is simple.

\subsection*{P11.5} Prove the Jordan-Hölder theorem. \textit{Hint: Induct on \( \min\{ \ell : M \text{ has a composition series of length } \ell \} \).}









\chapter{Noetherian Rings}

Due to the Jordan-Hölder theorem, finite-length modules are well-behaved and ideal for study. However, many naturally occurring modules do not have finite length.

A weaker condition than “finite length” is “finitely generated,” which applies to many more modules. Over a general ring, finitely generated modules can be challenging, but they are more manageable over Noetherian rings.

Let \( R \) be a ring. Consider the following conditions on \( R \).

\section{Equivalent Conditions for Noetherian Rings}

\begin{itemize}
    \item \textbf{Condition 1(a):} Every left ideal \( I \) of the ring \( R \) is finitely generated.
    \item \textbf{Condition 2(a):} For any chain of left ideals \( I_1 \subseteq I_2 \subseteq I_3 \subseteq \cdots \) in \( R \), we have \( I_r = I_{r+1} \) for all sufficiently large \( r \).
    \item \textbf{Condition 3(a):} Given any nonempty collection \( X \) of left ideals of \( R \), there exists some \( I \in X \) that is not properly contained in any other \( I' \in X \).
    
    \item \textbf{Condition 1(b):} Every left \( R \)-submodule \( M \) of \( R^n \) is finitely generated.
    \item \textbf{Condition 2(b):} For any chain of left \( R \)-submodules \( M_1 \subseteq M_2 \subseteq M_3 \subseteq \cdots \) of \( R^n \), we have \( M_r = M_{r+1} \) for all sufficiently large \( r \).
    \item \textbf{Condition 3(b):} Given any nonempty collection \( X \) of left \( R \)-submodules of \( R^n \), there exists some \( M \in X \) that is not properly contained in any other \( M' \in X \).

    \item \textbf{Condition 1(c):} For any finitely generated left \( R \)-module \( S \), every left \( R \)-submodule \( M \) of \( S \) is finitely generated.
    \item \textbf{Condition 2(c):} For any finitely generated left \( R \)-module \( S \), and for any chain of left \( R \)-submodules \( M_1 \subseteq M_2 \subseteq M_3 \subseteq \cdots \) of \( S \), we have \( M_r = M_{r+1} \) for all sufficiently large \( r \).
    \item \textbf{Condition 3(c):} For any finitely generated left \( R \)-module \( S \), given any nonempty collection \( X \) of left \( R \)-submodules of \( S \), there exists some \( M \in X \) that is not properly contained in any other \( M' \in X \).
\end{itemize}

\subsection*{P12.1} Prove that all these conditions are equivalent.

\begin{definition}
A ring that satisfies these conditions is called \textit{left Noetherian}. A ring that satisfies these conditions with “right” in place of “left” is called \textit{right Noetherian}. A ring that is both left and right Noetherian is called \textit{Noetherian}.
\end{definition}

\begin{remark}
These conditions are named after Emmy Noether (1882-1935), a German mathematician who is widely considered one of the greatest algebraists of all time.
\end{remark}







\chapter{Unique Factorization Domains (UFDs)}

Throughout this worksheet, let \( R \) be an integral domain.

\begin{definition}
Let \( r \) be an element of \( R \). We say that \( r \) is \textit{composite} if \( r \) is nonzero and can be written as a product of two non-units. We say that \( r \) is \textit{irreducible} if it is neither composite, nor zero, nor a unit. Thus, every element of \( R \) is described by precisely one of the adjectives “zero,” “unit,” “composite,” or “irreducible.”
\end{definition}

\begin{definition}
Let \( p \in R \). We say that \( p \) is \textit{prime} if \( pR \) is a prime ideal and \( p \neq 0 \).
\end{definition}

\section{Prime and Irreducible Elements}

\subsection*{P13.1} Let \( p \) be a nonzero, non-unit. Show that \( p \) is prime if and only if, whenever \( p \mid ab \), either \( p \mid a \) or \( p \mid b \).

\subsection*{P13.2} Show that prime elements are irreducible.

\subsection*{P13.3} Let \( k \) be a field, and let \( k[t^2, t^3] \) be the subring of \( k[t] \) generated by \( t^2 \) and \( t^3 \).
\begin{enumerate}
    \item Check that \( t^2 \) and \( t^3 \) are irreducible in \( k[t^2, t^3] \).
    \item Show that \( t^2 \) and \( t^3 \) are not prime in \( k[t^2, t^3] \).
\end{enumerate}

\subsection*{P13.4} Consider the subring \( \mathbb{Z}[\sqrt{-5}] \) of \( \mathbb{C} \).
\begin{enumerate}
    \item Show that \( 2 \), \( 3 \), and \( 1 \pm \sqrt{-5} \) are irreducible in \( \mathbb{Z}[\sqrt{-5}] \). (Hint: Use the complex absolute value.)
    \item Show that \( 2 \), \( 3 \), and \( 1 \pm \sqrt{-5} \) are not prime in \( \mathbb{Z}[\sqrt{-5}] \). (Hint: Note that \( 2 \cdot 3 = (1 + \sqrt{-5})(1 - \sqrt{-5}) \).)
\end{enumerate}

\begin{definition}
We define two elements \( p \) and \( q \) of \( R \) to be \textit{associate} if there exists a unit \( u \) such that \( p = qu \). We define two factorizations \( p_1 p_2 \cdots p_m \) and \( q_1 q_2 \cdots q_n \) to be \textit{equivalent} if \( m = n \) and there is a permutation \( \sigma \in S_n \) such that \( p_j \) is associate to \( q_{\sigma(j)} \) for all \( j \).
\end{definition}

\subsection*{P13.5} Show that any nonzero, non-unit element of \( R \) has at most one factorization into prime elements, up to equivalence.

\subsection*{P13.6} Give examples in the rings \( k[t^2, t^3] \) and \( \mathbb{Z}[\sqrt{-5}] \) of elements with multiple, non-equivalent factorizations into irreducible elements.

\section{Factorizations and Unique Factorization Domains}

\begin{definition}
We say that \( R \) has \textit{factorizations} if every nonzero, non-unit element in \( R \) can be written in at least one way as a product of irreducibles.
\end{definition}

\subsection*{P13.7} Let \( R \) have factorizations. Show that the following conditions are equivalent:
\begin{itemize}
    \item[(a)] All irreducible elements are prime.
    \item[(b)] Factorizations into irreducibles are unique, up to equivalence.
    \item[(c)] Every nonzero, non-unit element has a factorization into prime elements.
\end{itemize}

\begin{definition}
An integral domain that has factorizations and in which the equivalent conditions in Problem 13.7 hold is called a \textit{unique factorization domain}, or \textit{UFD}.
\end{definition}

\subsection*{P13.8} Let \( R \) be a Noetherian integral domain.
\begin{enumerate}
    \item Let \( r_1, r_2, r_3, \dots \) be a sequence of elements of \( R \) such that \( r_{j+1} \) divides \( r_j \) for all \( j \). Show that for sufficiently large \( j \), \( r_j \) and \( r_{j+1} \) are associates.
    \item Show that \( R \) has factorizations.
\end{enumerate}








\chapter{Principal Ideal Domains (PIDs)}

\begin{definition}
Let \( R \) be a commutative ring. An ideal \( I \) of \( R \) is called \textit{principal} if \( I = rR \) for some \( r \in R \).
\end{definition}

\subsection*{P14.1} Show that every ideal in \( \mathbb{Z} \) is principal. Do not assume unique factorization into primes. (Hint: Take the smallest positive element of the ideal.)

\begin{definition}
A \textit{Principal Ideal Domain} or \textit{PID} is an integral domain in which every ideal is principal.
\end{definition}

\section{Properties of PIDs}

\subsection*{P14.2} Show that every PID is Noetherian.

\subsection*{P14.3} Let \( R \) be a PID, and let \( u \) and \( v \) be two relatively prime elements of \( R \), meaning that if \( g \) divides both \( u \) and \( v \), then \( g \) is a unit. Show that \( u \) and \( v \) are comaximal, meaning that \( uR + vR = R \).

\subsection*{P14.4} Let \( R \) be a PID, let \( p \) be an irreducible element of \( R \), and let \( a \) be any element of \( R \). Show that either \( p \) divides \( a \) or else \( p \) and \( a \) are comaximal.

\subsection*{P14.5} Show that, in a PID, irreducible elements are prime.

\subsection*{P14.6} Show that a PID is a UFD.

\begin{remark}
We note, in particular, that we have now shown \( \mathbb{Z} \) is a UFD.
\end{remark}

\section{GCDs and Maximal Ideals in PIDs}

\subsection*{P14.7} Since PIDs are UFDs, we can talk about GCDs in them. Show that if \( R \) is a PID and \( a, b \in R \), then \( aR + bR = \text{GCD}(a, b)R \).

\subsection*{P14.8} Suppose \( R \) is a PID. Show that every nonzero prime ideal in \( R \) is a maximal ideal.

\section{Matrices over PIDs}

\subsection*{P14.9} Let \( R \) be a PID and let \( x, y \in R \). Show that there exists a matrix
\[
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
\]
with entries in \( R \) and determinant 1 such that
\[
\begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} \text{GCD}(x, y) \\ 0 \end{bmatrix}.
\]

\subsection*{P14.10} Let \( R \) be a PID, and let \( x, y \in R \). Show that there exist \( 2 \times 2 \) matrices \( U \) and \( V \) with entries in \( R \) and determinant 1 such that
\[
U \begin{bmatrix} x & 0 \\ 0 & y \end{bmatrix} V = \begin{bmatrix} \text{GCD}(x, y) & 0 \\ 0 & \text{LCM}(x, y) \end{bmatrix},
\]
where \( \text{LCM}(x, y) := \frac{xy}{\text{GCD}(x, y)} \).










\chapter{The Euclidean Algorithm}

\begin{remark}
To find the greatest common measure of two numbers... (Euclid, \textit{The Elements}, Book VII, Proposition 2)
\end{remark}

Starting with two positive integers \( x_0 \) and \( x_1 \), the Euclidean algorithm recursively defines two sequences of integers \( x_0, x_1, x_2, \dots \) and \( a_1, a_2, a_3, \dots \) as follows: For \( n \geq 2 \), we have
\[
x_n = x_{n-2} - a_{n-1} x_{n-1} \quad \text{with} \quad 0 \leq x_n < x_{n-1}.
\]
The algorithm terminates when \( x_n = 0 \).


\section{Euclidean Algorithm}
\subsection*{P15.1} Compute the sequences \( x_n \) and \( a_n \) with \( x_0 = 321 \) and \( x_1 = 123 \).

\subsection*{P15.2} Show that \( \text{GCD}(x_0, x_1) = \text{GCD}(x_1, x_2) = \dots = \text{GCD}(x_{n-1}, x_n) = x_{n-1} \) where \( x_n = 0 \). Let this common GCD be \( g \).

\subsection*{P15.3} Show that there is an elementary matrix \( E \) with \( E \begin{bmatrix} x_{n-2} \\ x_{n-1} \end{bmatrix} = \begin{bmatrix} x_n \\ x_{n-1} \end{bmatrix} \). Recall that a \( 2 \times 2 \) elementary matrix is of the form \( \begin{bmatrix} 1 & * \\ 0 & 1 \end{bmatrix} \) or \( \begin{bmatrix} 1 & 0 \\ * & 1 \end{bmatrix} \).

\subsection*{P15.4} Show that there is a product of elementary matrices \( F \) with \( F \begin{bmatrix} x_0 \\ x_1 \end{bmatrix} = \begin{bmatrix} g \\ 0 \end{bmatrix} \).

\subsection*{P15.5} Show that there exist sequences \( b_k \) and \( c_k \) such that \( b_k x_k + c_k x_{k+1} = g \) and describe how to compute \( b \)'s and \( c \)'s using \( a \)'s.

\subsection*{P15.6} Demonstrate your method by finding \( b \) and \( c \) such that \( b \cdot 321 + c \cdot 123 = 3 \).

\chapter{Euclidean Rings}

\begin{definition}
Suppose \( R \) is an integral domain. A \textit{norm} on \( R \) is any function \( N : R \to \mathbb{Z}_{\geq 0} \). The function \( N \) is said to be a \textit{positive norm} if \( N(r) > 0 \) for all nonzero \( r \). We call \( N \) a \textit{multiplicative norm} if \( N(ab) = N(a)N(b) \).
\end{definition}

Some examples:
\begin{itemize}
    \item The normal absolute value on \( \mathbb{Z} \) is a positive norm.
    \item The norm map \( N(a + bi) = a^2 + b^2 \) on the Gaussian Integers \( \mathbb{Z}[i] \) is a positive norm.
    \item If \( k \) is a field, we can define a norm on \( k[x] \) by \( N(p(x)) = \deg(p) \) for \( p \neq 0 \) and \( N(0) = 0 \).
    \item Alternatively, we can define a positive, multiplicative norm on \( k[x] \) by choosing some positive integer \( c \geq 2 \) and defining \( N(p) = c^{\deg(p)} \) for \( p \neq 0 \) and \( N(0) = 0 \).
\end{itemize}

\begin{definition}
An integral domain \( R \) is called a \textit{Euclidean Domain} if there exists a positive norm \( N \) on \( R \) such that for any two elements \( a, b \in R \) with \( b \neq 0 \), there exist \( q, r \in R \) with
\[
a = bq + r \quad \text{and} \quad N(r) < N(b).
\]
The element \( q \) is called the \textit{quotient} and \( r \) the \textit{remainder} of the division.
\end{definition}

\section{Properties and Examples of Euclidean Rings}

\subsection*{P16.1} Let \( k \) be a field. Show that \( k \) is Euclidean with respect to the norm where \( N(0) = 0 \) and \( N(x) = 1 \) for \( x \neq 0 \).

\subsection*{P16.2} Let \( k \) be a field. Verify that \( k[x] \) is Euclidean with respect to the norm \( N(p) = c^{\deg(p)} \) discussed above.

\subsection*{P16.3} Let \( R \) be an integral domain with positive, multiplicative norm \( N \), and let \( K \) be its field of fractions. For \( \frac{a}{b} \in K \), define \( N_K\left(\frac{a}{b}\right) = \frac{N(a)}{N(b)} \).
\begin{enumerate}
    \item Show that \( N_K \) is a well-defined function \( K \to \mathbb{Q}_{\geq 0} \).
    \item Show that \( R \) is Euclidean with respect to \( N \) if and only if, for each \( x \in K \), there exists \( q \in R \) such that \( N_K(x - q) < 1 \).
\end{enumerate}

\subsection*{P16.4} Verify that \( \mathbb{Z}[i] \) is Euclidean with respect to the norm \( N(a + bi) = a^2 + b^2 \).

\subsection*{P16.5} Show that every Euclidean domain is a PID.

\section{Additional Problems on Euclidean Domains}

\subsection*{P16.6} Show that \( \mathbb{Z}[\sqrt{-2}] \) is Euclidean with respect to the norm \( N(a + b\sqrt{-2}) = a^2 + 2b^2 \).

\subsection*{P16.7} Show that \( \mathbb{Z}[\sqrt{-3}] \) is not Euclidean with respect to the norm \( N(a + b\sqrt{-3}) = a^2 + 3b^2 \), but that \( \mathbb{Z}\left[\frac{1 + \sqrt{-3}}{2}\right] \) is Euclidean with respect to the norm \( N\left(c + d \frac{\sqrt{-3}}{2}\right) = \frac{c^2 + 3d^2}{4} \).

\subsection*{P16.8} Let \( p \) be a positive prime integer.
\begin{enumerate}
    \item Show that \( \mathbb{Z}[i] \) has an ideal \( \pi \) with \( \#(\mathbb{Z}[i]/\pi) = p \) if and only if there exists a square root of \( -1 \) in \( \mathbb{Z}/p\mathbb{Z} \).
    \item Show that \( \mathbb{Z}[i] \) has a principal ideal \( (a + bi)\mathbb{Z}[i] \) with \( \mathbb{Z}[i]/(a + bi)\mathbb{Z}[i] \) if and only if \( p \) is of the form \( a^2 + b^2 \).
    \item Conclude that a prime \( p \) is of the form \( a^2 + b^2 \) if and only if there exists a square root of \( -1 \) in \( \mathbb{Z}/p\mathbb{Z} \).
\end{enumerate}

\subsection*{P16.9} Let \( R \) be a Euclidean domain. Show that there exists a nonunit \( f \) such that every nonzero residue class in \( R/fR \) is represented by a unit of \( R \). Deduce that \( \mathbb{Z}\left[\frac{1 + \sqrt{-19}}{2}\right] \) is not Euclidean for any norm function.








\chapter{Introduction to Smith Normal Form}

\begin{theorem}[Smith Normal Form]
Let \( R \) be a principal ideal domain and let \( X \) be an \( m \times n \) matrix with entries in \( R \).
Then there exist invertible \( m \times m \) and \( n \times n \) matrices \( U \) and \( V \), and elements \( d_1, d_2, \ldots, d_{\min(m,n)} \) of \( R \), such that
\[
X = U D V,
\]
where \( D \) is the \( m \times n \) matrix with \( D_{jj} = d_j \) and \( D_{ij} = 0 \) for \( i \neq j \). Moreover, we may assume \( d_1 | d_2 | \cdots | d_{\min(m,n)} \), and with this normalization, the \( d_j \) are unique up to multiplication by units.
\end{theorem}

The \( d_j \) are called the \textit{invariant factors} of \( X \). We first set up some notation:

\section{Equivalence Relation on Matrices}

\subsection*{P17.1}
Let \( R \) be any ring. Define a relation \( \sim \) on \( \text{Mat}_{m \times n}(R) \) by \( X \sim Y \) if there exist invertible \( m \times m \) and \( n \times n \) matrices \( U \) and \( V \) with \( Y = U X V \). Show that \( \sim \) is an equivalence relation.

\subsection*{P17.2}
Here is a more abstract perspective on \( \sim \): Let \( X \) and \( Y \in \text{Mat}_{m \times n}(R) \).
\begin{itemize}
    \item[(1)] Show that \( X \sim Y \) if and only if we can choose vertical isomorphisms making the following diagram commute:
    \[
    \begin{array}{ccc}
        R^n & \xrightarrow{X} & R^m \\
        \cong \downarrow & & \downarrow \cong \\
        R^n & \xrightarrow{Y} & R^m
    \end{array}
    \]
    \item[(2)] Show that, if \( X \sim Y \), then the kernels, cokernels, and images of \( X \) and \( Y \) are isomorphic \( R \)-modules.
\end{itemize}

For nonnegative integers \( m \) and \( n \) and elements \( d_1, d_2, \ldots, d_{\min(m,n)} \) of \( R \), we define \( \text{diag}_{m,n}(d_1, d_2, \ldots, d_{\min(m,n)}) \) to be the \( m \times n \) matrix \( D \) as above. Thus, Smith normal form says that every matrix is \( \sim \)-equivalent to a matrix of the form \( \text{diag}_{m,n}(d_1, d_2, \ldots, d_{\min(m,n)}) \) with \( d_1 | d_2 | \cdots | d_{\min(m,n)} \), where the \( d_j \) are unique up to multiplication by units.

\section{The Cauchy-Binet Formula}

\begin{theorem}[The Cauchy-Binet Formula]
Let \( R \) be a commutative ring. Given an \( m \times n \) matrix \( X \) with entries in \( R \), and subsets \( I \subseteq \{1, 2, \ldots, m\} \) and \( J \subseteq \{1, 2, \ldots, n\} \) of the same size, define \( \Delta_{IJ}(X) \) to be the determinant of the square submatrix of \( X \) using rows \( I \) and columns \( J \). Let \( X \) and \( Y \) be \( a \times b \) and \( b \times c \) matrices with entries in \( R \), and let \( I \) and \( K \) be subsets of \( \{1, 2, \ldots, a\} \) and \( \{1, 2, \ldots, c\} \) with \( |I| = |K| = q \). Then
\[
\Delta_{IK}(XY) = \sum_{J \subseteq \{1,2,\ldots,b\}, |J|=q} \Delta_{IJ}(X) \Delta_{JK}(Y).
\]
\end{theorem}

The next few problems show how to compute invariant factors.

\section{Properties of Minors and Invariant Factors}

\subsection*{P17.3}
Let \( R \) be a UFD. Let \( U \), \( X \), and \( V \) be \( m \times m \), \( m \times n \), and \( n \times n \) matrices with entries in \( R \). Show that the GCD of the \( q \times q \) minors of \( X \) divides the GCD of the \( q \times q \) minors of \( U X V \).

\subsection*{P17.4}
Let \( R \) be a UFD. Show that, if \( X \sim Y \), then the GCD of the \( q \times q \) minors of \( X \) is equal to the GCD of the \( q \times q \) minors of \( Y \).

\subsection*{P17.5}
Let \( R \) be a UFD. Let \( X \) be an \( m \times n \) matrix with entries in \( R \). Show that, if \( X \sim \text{diag}_{m,n}(d_1, d_2, \ldots, d_{\min(m,n)}) \) with \( d_1 | d_2 | \cdots | d_{\min(m,n)} \), then \( d_1 d_2 \cdots d_q \) is the GCD of the \( q \times q \) minors of \( X \).

\section{Examples and the Cauchy-Binet Proof}

\subsection*{P17.6}
Assuming the Smith normal form theorem for \( \mathbb{Z} \), compute the invariant factors of the following matrices:
\[
\begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}, \quad \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}, \quad \begin{bmatrix} 2 & -1 & -1 \\ -1 & 2 & -1 \\ -1 & -1 & 2 \end{bmatrix}.
\]

\subsection*{P17.7}
If you have gotten this far, go ahead and prove the Cauchy-Binet formula. It can be done by brute force.

\begin{remark}
The factorization \( U D V \) may remind the reader of singular value decomposition. This is not a coincidence; Smith normal form can be thought of as a non-Archimedean version of singular value decomposition.
\end{remark}











\chapter{Proof of the Smith Normal Form Theorem}

Most people find the proof of the Smith normal form theorem for Euclidean domains more intuitive than for a general PID. When I went to write them out, they actually came out very similar.

\section{Smith Normal Form for Euclidean Integral Domains}

\subsection*{P18.1} (\textbf{Proof of Smith normal form for Euclidean integral domains}) Let \( R \) be a Euclidean integral domain with positive norm \( N \). Let \( X \in \text{Mat}_{m \times n}(R) \). If \( X = 0 \), the Smith normal form theorem clearly holds for \( X \), so assume otherwise. Let \( d \) be an element of smallest norm among all nonzero elements occurring as an entry in a matrix \( Y \) with \( Y \sim X \). Let \( Y \) be a matrix with \( Y \sim X \) and \( Y_{11} = d \).
\begin{enumerate}
    \item Show that \( d \) divides \( Y_{i1} \) and \( Y_{1j} \) for all \( 2 \leq i \leq m \) and \( 2 \leq j \leq n \).
    \item Show that there is a matrix \( Z \sim Y \) with \( Z_{11} = d \) and \( Z_{i1} = Z_{1j} = 0 \) for all \( 2 \leq i \leq m \) and \( 2 \leq j \leq n \).
    \item Show that \( d \) divides \( Z_{ij} \) for all \( 2 \leq i \leq m \) and \( 2 \leq j \leq n \).
    \item Show that \( X \) is \( \sim \)-equivalent to a matrix of the form \( \operatorname{diag}_{m \times n}(d_1, d_2, \dots, d_{\min(m,n)}) \) with \( d_1 | d_2 | \dots | d_{\min(m,n)} \).
\end{enumerate}

\subsection*{P18.2} (\textbf{Consequence of the proof of Smith normal form for Euclidean integral domains}) Define a stronger equivalence relation \( \sim_E \) where \( X \sim_E Y \) if \( Y = UXV \) where \( U \) and \( V \) are products of elementary matrices.
\begin{enumerate}
    \item Trace through your proof and verify that, in a Euclidean integral domain, every matrix is \( \sim_E \)-equivalent to a matrix of the form \( \operatorname{diag}_{m \times n}(d_1, d_2, \dots, d_{\min(m,n)}) \) with \( d_1 | d_2 | \dots | d_{\min(m,n)} \).
    \item Let \( R \) be a Euclidean integral domain. Let \( \operatorname{SL}_n(R) \) be the group of \( n \times n \) matrices with entries in \( R \) and determinant 1. Show that \( \operatorname{SL}_n(R) \) is generated by elementary matrices.
\end{enumerate}

\section{Smith Normal Form for General Principal Ideal Domains (PIDs)}

\subsection*{P18.3} Let \( R \) be a Noetherian ring (such as a PID), and let \( D \) be a nonempty subset of \( R \). Show that there is an element \( d \in D \) which is \textit{minimal with respect to division}: More precisely, show that there exists an element such that if \( d' \in D \) divides \( d \), then \( d \) divides \( d' \) as well.

\subsection*{P18.4} (\textbf{Proof of Smith normal form for PIDs}) Let \( R \) be a PID and let \( X \in \text{Mat}_{m \times n}(R) \). Let \( D \) be the set of all entries occurring in any matrix \( Y \) with \( Y \sim X \). Let \( d \) be as in Problem 18.3 for \( D \), and let \( Y \) be a matrix with \( Y \sim X \) and \( Y_{11} = d \).
\begin{enumerate}
    \item Show that \( d \) divides \( Y_{i1} \) and \( Y_{1j} \) for all \( 2 \leq i \leq m \) and \( 2 \leq j \leq n \).
    \item Show that there is a matrix \( Z \sim Y \) with \( Z_{11} = d \) and \( Z_{i1} = Z_{1j} = 0 \) for all \( 2 \leq i \leq m \) and \( 2 \leq j \leq n \).
    \item Show that \( d \) divides \( Z_{ij} \) for all \( 2 \leq i \leq m \) and \( 2 \leq j \leq n \).
    \item Show that \( X \) is \( \sim \)-equivalent to a matrix of the form \( \operatorname{diag}_{m \times n}(d_1, d_2, \dots, d_{\min(m,n)}) \) with \( d_1 | d_2 | \dots | d_{\min(m,n)} \).
\end{enumerate}










\chapter{Classification of Finitely Generated Modules over a PID}

\section{Structure of Finitely Generated Modules}

\subsection*{P19.1} Let \( S \) be a commutative ring, and let \( M \) be a finitely generated \( S \)-module.
\begin{enumerate}
    \item Show that there is a surjection \( S^{\oplus m} \twoheadrightarrow M \) for some \( m \).
    \item Suppose that \( S \) is Noetherian (for example, every PID is Noetherian). Show that there is a surjection \( S^n \twoheadrightarrow \ker(S^m \to M) \) for some \( n \).
    \item With the hypotheses and assumptions from the previous part, show that there exists an \( m \times n \) matrix \( X \) with \( M \cong S^m / X S^n \).
\end{enumerate}

The previous problem shows that every finitely generated \( S \)-module is of the form \( S^m / X S^n \) for some \( m \times n \) matrix \( X \). Now, and throughout this worksheet, let \( R \) be a PID. We will explore the structure of \( R^m / X R^n \) in terms of the Smith normal form of \( X \).

\section{Invariant Factors and Kernel Structure}

\subsection*{P19.2} Let \( X \in \operatorname{Mat}_{m \times n}(R) \), and let \( (d_1, d_2, \dots, d_{\min(m,n)}) \) be the invariant factors of \( X \).
\begin{enumerate}
    \item Show that \( R^m / X R^n \cong R^{m - \min(m,n)} \oplus \bigoplus_j R / d_j R \).
    \item Show that \( \ker(X) \cong R^{\#\{j : d_j = 0\} + n - \min(m,n)} \).
\end{enumerate}

\section{Classification Forms of Modules over a PID}

\subsection*{P19.3} (\textbf{Classification of modules over a PID: Elementary divisor form}) Show that every finitely generated \( R \)-module \( M \) is of the form \( \bigoplus R / d_j R \) for some nonunits \( d_1, d_2, \dots, d_k \in R \) with \( d_1 | d_2 | \cdots | d_k \).

\subsection*{P19.4} (\textbf{Classification of modules over a PID: Prime power form}) Show that every finitely generated \( R \)-module \( M \) is of the form \( R^{\oplus r} \oplus \bigoplus R / p_j^{e_j} R \) for some nonnegative integer \( r \), a sequence of prime elements \( p_j \), and a sequence of positive integers \( e_j \).

Problems 19.3 and 19.4 each provide a list of modules such that every finitely generated \( R \)-module \( M \) is isomorphic to some module in the list. To ensure a full classification, we now address verifying that these lists do not contain duplicate isomorphism classes.

\section{Uniqueness of Classification Forms}

\subsection*{P19.5} Let \( q \) be a prime element of \( R \), and let \( M \) be an \( R \)-module.
\begin{enumerate}
    \item Show that \( R / q R \) is a field, and that for any \( k \geq 0 \), \( q^k M / q^{k+1} M \) is an \( R / q R \)-vector space.
    \item Let \( M = R^{\oplus r} \oplus \bigoplus R / p_j^{e_j} R \) as in Problem 19.4. Provide a formula for the dimension of \( q^k M / q^{k+1} M \) as an \( R / q R \)-vector space in terms of the \( e_j \) and \( r \).
    \item Suppose that \( R^{\oplus r} \oplus \bigoplus R / p_j^{e_j} R \cong R^{\oplus r'} \oplus \bigoplus R / p_j^{e_j'} R \). Show that \( r = r' \) and \( e_j = e_j' \).
\end{enumerate}

\subsection*{P19.6} Let \( d_1, d_2, \dots, d_k \) and \( d'_1, d'_2, \dots, d'_{k'} \) be nonunits of \( R \) with \( d_1 | d_2 | \dots | d_k \) and \( d'_1 | d'_2 | \dots | d'_{k'} \), such that \( \bigoplus R / d_i R \cong \bigoplus R / d'_i R \). Show that \( k = k' \) and \( d_i \) is associate to \( d'_i \).











\chapter{Applications of Jordan Normal Form and Rational Canonical Form}

The purpose of this worksheet is to provide examples of problems where knowing Jordan Normal Form is useful.

\section{Characteristic and Minimal Polynomials}

\subsection*{P20.1} Let \( A \) be a \( 5 \times 5 \) complex matrix with minimal polynomial \( X^5 - X^3 \).
\begin{enumerate}
    \item What is the characteristic polynomial of \( A^2 \)?
    \item What is the minimal polynomial of \( A^2 \)?
\end{enumerate}

\section{Square Roots of Matrices}

\subsection*{P20.2} In this problem, we investigate square roots of matrices:
\begin{enumerate}
    \item Let \( g \in \operatorname{GL}_n(\mathbb{C}) \). Show that there exists an \( h \in \operatorname{GL}_n(\mathbb{C}) \) with \( h^2 = g \).
    \item Show that there is no matrix \( h \in \operatorname{GL}_2(\mathbb{R}) \) with \( h^2 = \begin{bmatrix} -1 & 1 \\ 0 & -1 \end{bmatrix} \).
\end{enumerate}

\section{Jordan-Chevalley Decomposition}

\subsection*{P20.3} Let \( k \) be an algebraically closed field and let \( A \) be an \( n \times n \) matrix with entries in \( k \). Show that \( A \) can be written in the form \( D + N \), where \( D \) is diagonalizable, \( N \) is nilpotent, and \( DN = ND \). This decomposition is known as the Jordan-Chevalley decomposition of \( A \).

\section{Regular Matrices and Centralizers}

\subsection*{P20.4} Let \( k \) be an algebraically closed field, and let \( A \) be an \( n \times n \) matrix with entries in \( k \). Define
\[
k[A] = \operatorname{span}_k(1, A, A^2, A^3, \dots) \subset \operatorname{Mat}_{n \times n}(k),
\]
and
\[
Z(A) = \{ B \in \operatorname{Mat}_{n \times n}(k) : AB = BA \}.
\]
\begin{enumerate}
    \item Show that \( k[A] \subseteq Z(A) \). (Jordan form is not recommended here.)
    \item Show that the following are equivalent:
    \begin{enumerate}
        \item \( \dim_k k[A] = n \).
        \item \( \dim_k Z(A) = n \).
        \item \( k[A] = Z(A) \).
        \item The minimal polynomial of \( A \) is the same as the characteristic polynomial of \( A \).
        \item For each eigenvalue \( \lambda \) of \( A \), there is only one Jordan block of \( A \).
    \end{enumerate}
    A matrix satisfying these conditions is called \textit{regular}.
    \item For any matrix \( A \), show that \( \dim k[A] \leq n \).
    \item For any matrix \( A \), show that \( \dim Z(A) \geq n \).
\end{enumerate}

\section{Diagonalizability of Real Symmetric Matrices}

\subsection*{P20.5} Let’s prove that a real symmetric matrix is diagonalizable!
\begin{enumerate}
    \item Let \( X \) be an \( n \times n \) real matrix and suppose that \( X \) is not diagonalizable. Prove that there exists a two-dimensional subspace \( V \) of \( \mathbb{R}^n \) such that \( X \) maps \( V \) to itself by a matrix of the form
    \[
    \begin{bmatrix} 0 & -c \\ 1 & -b \end{bmatrix}
    \]
    with \( b^2 - 4c \leq 0 \). (Hint for handling a technical issue: Notice that the matrices \( \begin{bmatrix} \lambda & 0 \\ 1 & \lambda \end{bmatrix} \) and \( \begin{bmatrix} 0 & -\lambda^2 \\ 1 & 2\lambda \end{bmatrix} \) are similar.)
    \item Now suppose that \( X \) is symmetric. Let \( \cdot \) denote the ordinary dot product on \( \mathbb{R}^n \). Show that, for any \( v, w \in \mathbb{R}^n \), we have \( (Xv) \cdot w = v \cdot (Xw) \).
    \item Now suppose that \( X \) is symmetric and non-diagonalizable. Let \( V \) be the subspace in part (1), and let \( v, w \) be a basis for \( V \) on which \( X \) acts by the matrix \( \begin{bmatrix} 0 & -c \\ 1 & -b \end{bmatrix} \) with \( b^2 - 4c \leq 0 \). Show that \( w \cdot w + b(v \cdot w) + c(v \cdot v) = 0 \).
    \item Deduce a contradiction. (Hint: Recall the Cauchy-Schwarz inequality \( (v \cdot w)^2 \leq (v \cdot v)(w \cdot w) \).)
\end{enumerate}











\chapter{Unique Factorization in Polynomial Rings}

Let \( R \) be an integral domain and let \( F \) be its field of fractions. We know that \( F[x] \) is a Euclidean domain, hence a PID (Problem 16.5), and thus a UFD (Problem 14.6). Consequently, any polynomial \( p(x) \in R[x] \) factors uniquely in \( F[x] \). However, factorization in \( R[x] \) can be more complex:

\subsection*{P21.1} Let \( R = \mathbb{R}[t^2, t^3] \) and let \( F \) be the fraction field of \( R \). Show that the polynomial \( x^2 - t^2 \) factors in \( F[x] \), but is irreducible in \( R[x] \).

\subsection*{P21.2} Let \( R = \mathbb{R}[t^2, t^3] \) and let \( F \) be the fraction field of \( R \). Provide two different irreducible factorizations of the polynomial \( x^6 - t^6 \) over \( R[x] \).

For the remainder of this worksheet, assume that \( R \) is a UFD, simplifying factorization in \( R[x] \).

\section{Primality and Irreducibility in Polynomial Rings}

\subsection*{P21.3} Let \( p \in R \) be a prime element. Let \( a(x) \) and \( b(x) \) be polynomials in \( R[x] \). Show that if \( a(x)b(x) \in pR[x] \), then either \( a(x) \in pR[x] \) or \( b(x) \in pR[x] \).

\begin{definition}
A polynomial \( a_n x^n + \dots + a_1 x + a_0 \) in \( R[x] \) is called \textit{primitive} if \( \text{GCD}(a_n, \dots, a_1, a_0) = 1 \).
\end{definition}

\subsection*{P21.4} (\textbf{Gauss’s Lemma}) Let \( a(x) b(x) = c(x) \) with \( a(x), b(x), c(x) \in R[x] \). Show that \( c(x) \) is primitive if and only if both \( a(x) \) and \( b(x) \) are primitive.

\subsection*{P21.5} Let \( a(x) b(x) = c(x) \) with \( a(x) \in R[x] \) primitive, \( b(x) \in F[x] \), and \( c(x) \in R[x] \). Show that \( b(x) \in R[x] \).

\subsection*{P21.6} Let \( p(x) \in R[x] \). Show that the following conditions are equivalent:
\begin{enumerate}
    \item \( p(x) \) is prime in \( R[x] \).
    \item \( p(x) \) is irreducible in \( R[x] \).
    \item One of the following holds:
    \begin{itemize}
        \item \( p(x) \) is a constant polynomial whose value is a prime element \( p \) of \( R \).
        \item \( p(x) \) is primitive in \( R[x] \) and is prime in \( F[x] \).
    \end{itemize}
\end{enumerate}

\begin{remark}
Recall that \( R \) and \( F[x] \) are UFDs, so the terms “prime” and “irreducible” are interchangeable in these rings.
\end{remark}

\section{Unique Factorization in \( R[x] \)}

\subsection*{P21.7} Show that if \( R \) is a UFD, then \( R[x] \) is also a UFD.

\begin{remark}
This result implies that \( \mathbb{Z}[x_1, \dots, x_n] \) and \( k[x_1, \dots, x_n] \) are UFDs for any field \( k \) and any number of variables.
\end{remark}








\chapter{Some Problems About Exterior Algebra}

\section{Basic Properties of Exterior Algebra}

\subsection*{P22.1}
Let \( e_1, e_2, e_3 \) be the standard basis of \( \mathbb{R}^3 \). Expand
\[
(e_1 + e_2 + e_3) \wedge (e_1 + 2e_2 + 3e_3)
\]
in the basis \( e_1 \wedge e_2 \), \( e_1 \wedge e_3 \), \( e_2 \wedge e_3 \) of \( \mathbb{R}^3 \).

\subsection*{P22.2}
Let \( L : \mathbb{C}^n \rightarrow \mathbb{C}^n \) be a linear map with eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_n \). What are the eigenvalues of \( \wedge^2 L \)? Of \( \wedge^k L \)?

\section{Conditions for the Wedge Product to Vanish}

\subsection*{P22.3}
Let \( v_1, v_2, \ldots, v_d \) be vectors in a vector space \( V \). Show that \( v_1 \wedge v_2 \wedge \cdots \wedge v_d = 0 \) if and only if the \( v_i \) are linearly dependent.

\subsection*{P22.4}
Let \( V \) be a vector space over a field \( k \) and let \( \eta \in \wedge^d V \) for \( d > 0 \).
\begin{itemize}
    \item[(1)] Let \( v \) be a nonzero vector in \( V \). Show that \( v \wedge \eta = 0 \) if and only if \( \eta \) can be factored as \( v \wedge \theta \) for \( \theta \in \wedge^{d-1} V \).
    \item[(2)] More generally, let \( U = \{v \in V : v \wedge \eta = 0\} \) and let \( u_1, u_2, \ldots, u_k \) be a basis of \( U \). Show that \( \eta \) can be factored as \( u_1 \wedge u_2 \wedge \cdots \wedge u_k \wedge \psi \) for some \( \psi \in \wedge^{d-k} V \).
\end{itemize}

\section{Cross Product and Isomorphisms in \(\mathbb{R}^3\)}

\subsection*{P22.5}
Let \( e_1, e_2, e_3 \) be the standard basis of \( \mathbb{R}^3 \).
\begin{itemize}
    \item[(1)] Show that there is a unique isomorphism \( h : \wedge^2 \mathbb{R}^3 \rightarrow \mathbb{R}^3 \) such that, for \( v \in \mathbb{R}^3 \) and \( \eta \in \wedge^2 \mathbb{R}^3 \), we have
    \[
    v \wedge \eta = (v \cdot h(\eta)) e_1 \wedge e_2 \wedge e_3.
    \]
    Here \( \cdot \) denotes the standard dot product.
    \item[(2)] The cross product map \( V \times V \rightarrow V \) is defined by \( v \times w := h(v \wedge w) \). Check that this is the cross product you already know.
    \item[(3)] Let \( g \in SO(3) \). Show that \( g h(\eta) = h(\wedge^2(g) \eta) \) and that \( g(u \times v) = g(u) \times g(v) \).
\end{itemize}

\section{Adjugate Matrix and Exterior Powers}

\subsection*{P22.6}
Let \( V \) be a vector space of dimension \( n \). Let \( L : V \rightarrow V \) be a linear map; we will also write \( L \) for the matrix of \( L \). Recall that the adjugate matrix, \( \text{Adj}(L) \), is the matrix whose \( (i, j) \) entry is \( (-1)^{i+j} \) times the determinant of the \( (n-1) \times (n-1) \) minor of \( L \) where we delete row \( j \) and column \( i \). For example,
\[
\text{Adj} \begin{bmatrix} r & s & t \\ u & v & w \\ x & y & z \end{bmatrix} = \begin{bmatrix} vz - wy & -(sz - ty) & sw - tv \\ -(uz - wx) & rz - tx & -(rw - tu) \\ uy - vx & -(ry - sx) & rv - su \end{bmatrix}.
\]
\begin{itemize}
    \item[(1)] What is the relation between \( \text{Adj}(L) \) and \( \wedge^{n-1}(L) \)?
    \item[(2)] For any \( v \in V \) and \( \eta \in \wedge^{n-1}(V) \), show that \( L(v) \wedge \wedge^{n-1}(L)(\eta) = (\det L)(v \wedge \eta) \).
    \item[(3)] Show that \( L \, \text{Adj}(L) = (\det L) \text{Id}_n \).
\end{itemize}










\chapter{Summary of Major Results}

This worksheet allows you to review the major results covered in the previous worksheets. Throughout, let \( R \) be an integral domain. It is recommended first to identify all valid implications and then consider counterexamples to verify that other implications do not hold.

\section{Implications Between Ring Properties}

\subsection*{A.1} Draw arrows indicating which implications exist between the following concepts:
\begin{itemize}
    \item \( R \) is a PID.
    \item \( R \) is Euclidean.
    \item \( R \) is Noetherian.
    \item \( R \) is a UFD.
\end{itemize}

\section{Implications Between Ideal Properties}

\subsection*{A.2} Let \( I \) be a nonzero ideal of \( R \). Draw arrows indicating which implications exist between the following concepts:
\begin{itemize}
    \item \( I \) is prime.
    \item \( I \) is maximal.
    \item \( I \) is of the form \( (f) \) for \( f \) irreducible.
    \item \( I \) is of the form \( (f) \) for \( f \) prime.
\end{itemize}

\subsection*{A.3} Suppose that \( R \) is a UFD, and let \( I \) be a nonzero ideal of \( R \). Draw arrows indicating which implications exist between the following concepts:
\begin{itemize}
    \item \( I \) is prime.
    \item \( I \) is maximal.
    \item \( I \) is of the form \( (f) \) for \( f \) irreducible.
    \item \( I \) is of the form \( (f) \) for \( f \) prime.
\end{itemize}

\subsection*{A.4} Suppose that \( R \) is a PID, and let \( I \) be a nonzero ideal of \( R \). Draw arrows indicating which implications exist between the following concepts:
\begin{itemize}
    \item \( I \) is prime.
    \item \( I \) is maximal.
    \item \( I \) is of the form \( (f) \) for \( f \) irreducible.
    \item \( I \) is of the form \( (f) \) for \( f \) prime.
\end{itemize}









\chapter{Rational Canonical Form of a Matrix}

\subsection*{B.1} Let \( k \) be a field. Make sure everyone in your group remembers how to do the old homework problem: Give an equivalence between
\begin{enumerate}
    \item \( k[t] \)-modules which are finite dimensional as \( k \)-vector spaces, and
    \item pairs \( (V, T) \) where \( V \) is a finite-dimensional \( k \)-vector space and \( T : V \to V \) is a \( k \)-linear map.
\end{enumerate}

\subsection*{B.2} Let \( k \) be a field and let \( M_1 \) and \( M_2 \) be \( k[t] \)-modules which are finite dimensional as \( k \)-vector spaces, corresponding to \( (V_1, T_1) \) and \( (V_2, T_2) \). What is the pair corresponding to \( M_1 \oplus M_2 \)?

\begin{definition}
Let \( k \) be a field and let \( f = x^d + f_{d-1} x^{d-1} + \dots + f_0 \) be a monic polynomial with coefficients in \( k \). We define the \textit{companion matrix} of \( f \) by
\[
C(f) = \begin{bmatrix}
0 & 0 & \cdots & 0 & -f_0 \\
1 & 0 & \cdots & 0 & -f_1 \\
0 & 1 & \cdots & 0 & -f_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & \cdots & 1 & -f_{d-1}
\end{bmatrix}.
\]
\end{definition}

\subsection*{B.3} Show that \( k[x] / f(x)k[x] \) corresponds to the pair \( (k^d, C(f)) \).

\begin{definition}
An \( n \times n \) matrix with entries in \( k \) is said to be in \textit{rational canonical form} if it is a block matrix of the form
\[
\begin{bmatrix} C(f_1) & & \\ & C(f_2) & \\ & & \ddots & \\ & & & C(f_k) \end{bmatrix},
\]
for some monic polynomials \( f_1(x), f_2(x), \dots, f_k(x) \) with \( f_1 | f_2 | \dots | f_k \).
\end{definition}

\subsection*{B.4} (\textbf{The Rational Canonical Form Theorem}) Let \( V \) be a finite-dimensional \( k \)-vector space and let \( T : V \to V \) be a \( k \)-linear map. Show that there is a basis of \( V \) in which \( T \) is given by a matrix in rational canonical form, and that the polynomials \( f_1, f_2, \dots, f_k \) are uniquely determined by \( (V, T) \).

\subsection*{B.5} Describe the characteristic polynomial of \( T \) in terms of \( f_1, f_2, \dots, f_k \).

\subsection*{B.6} The minimal polynomial of \( T \) is the monic polynomial \( g(t) \in k[t] \) of lowest degree such that \( g(T) = 0 \). Describe the minimal polynomial of \( T \) in terms of \( f_1, f_2, \dots, f_k \).

\begin{remark}
The term “rational” here refers to the fact that we can put matrices into rational canonical form while staying in the same ground field, unlike Jordan canonical form, which may require passing to a larger field.
\end{remark}

\chapter{Jordan and Generalized Jordan Form of a Matrix}

\begin{definition}
Let \( \lambda \) be an element of \( k \). We define the \textit{Jordan block} \( J_n(\lambda) \) by
\[
J_n(\lambda) = \begin{bmatrix}
\lambda & 0 & \cdots & 0 \\
1 & \lambda & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \lambda
\end{bmatrix}.
\]
\end{definition}

\subsection*{C.1} Show that \( (x - \lambda)^{n-1}, (x - \lambda)^{n-2}, \dots, (x - \lambda), 1 \) is a basis for \( k[x] / (x - \lambda)^n k[x] \), and show that multiplication by \( x \), in this basis, is given by the matrix \( J_n(\lambda) \).

\begin{definition}
A matrix is said to be in \textit{Jordan normal form} if it is a block matrix whose blocks are Jordan blocks.
\end{definition}

\subsection*{C.2} (\textbf{The Jordan Normal Form Theorem}) Suppose that the field \( k \) is algebraically closed. Show that each \( n \times n \) matrix with entries in \( k \) is similar to a matrix in Jordan normal form, and that the Jordan normal form is unique up to reordering blocks.

\begin{definition}
Let \( f = x^d + f_{d-1} x^{d-1} + \dots + f_1 x + 0 \) be a monic polynomial with coefficients in \( k \). Let \( U_d \) be the \( d \times d \) matrix with a 1 in the upper-right corner and all other entries 0. Define the \textit{generalized Jordan block} \( J_n(f(x)) \) to be the \( (dn) \times (dn) \) block matrix
\[
J_n(f) = \begin{bmatrix}
C(f) & 0 & \cdots & 0 \\
U_d & C(f) & \cdots & 0 \\
0 & U_d & \cdots & 0 \\
\vdots & \vdots & \ddots & C(f)
\end{bmatrix}.
\]
\end{definition}

\subsection*{C.3} Show that \( \{ x^i f(x)^j : 0 \leq i < d, 0 \leq j < n \} \) is a basis for \( k[x] / f(x)^n k[x] \).

\subsection*{C.4} Show that multiplication by \( x \) in the above basis is given by the matrix \( J_n(f(x)) \).

\begin{definition}
A matrix is in \textit{generalized Jordan normal form} if it is a block diagonal matrix where each block is of the form \( J_{n_i}(p_i(x)) \) and the polynomials \( p_i(x) \) are irreducible.
\end{definition}

\subsection*{C.5} Show that each \( n \times n \) matrix with entries in \( k \) is similar to a matrix in generalized Jordan normal form, and that the generalized Jordan normal form is unique up to reordering blocks.









\chapter{Tensor Products of Vector Spaces}

“I wasn’t asking much: I just wanted to figure out the most basic properties of tensor products. And it seemed like a moral
issue. I felt strongly that if I really really wanted to feel like I understand this ring, which is after all a set, then at least I
should be able to tell you, with moral authority, whether an element is zero or not. For fuck’s sake!”
— Cathy O’Neil, “What tensor products taught me about living my life”

Let \( k \) be a field and let \( V \) and \( W \) be \( k \)-vector spaces. Define \( V \otimes W \) to be the \( k \)-vector space generated by symbols
\( v \otimes w \), for \( v \in V \) and \( w \in W \), modulo the following relations:

\[
v \otimes (w_1 + w_2) = v \otimes w_1 + v \otimes w_2, \quad (v_1 + v_2) \otimes w = v_1 \otimes w + v_2 \otimes w, \quad c(v \otimes w) = (cv) \otimes w = v \otimes (cw),
\]
where \( v, v_1, v_2 \in V \), \( w, w_1, w_2 \in W \), and \( c \in k \).

\subsection*{D.1} Show that \( 0 \otimes w = v \otimes 0 = 0 \).

\subsection*{D.2} Prove the universal property of tensor products: For any vector space \( X \) and any \( k \)-bilinear pairing \( \langle \cdot , \cdot \rangle : V \times W \to X \), there exists a unique \( k \)-linear map \( \lambda : V \otimes W \to X \) such that \( \langle v, w \rangle = \lambda(v \otimes w) \).

\subsection*{D.3} Let \( V_1, V_2, W_1, W_2 \) be \( k \)-vector spaces and \( \alpha : V_1 \to V_2 \) and \( \beta : W_1 \to W_2 \) be \( k \)-linear maps. Show that
there is a unique linear map \( \alpha \otimes \beta : V_1 \otimes W_1 \to V_2 \otimes W_2 \) such that \( (\alpha \otimes \beta)(v \otimes w) = \alpha(v) \otimes \beta(w) \).

\subsection*{D.4} Let \( V_1, V_2, V_3, W_1, W_2, W_3 \) be \( k \)-vector spaces and \( \alpha_1 : V_1 \to V_2 \), \( \alpha_2 : V_2 \to V_3 \), \( \beta_1 : W_1 \to W_2 \), and \( \beta_2 : W_2 \to W_3 \) be \( k \)-linear maps. Show that \( (\alpha_2 \otimes \beta_2) \circ (\alpha_1 \otimes \beta_1) = (\alpha_2 \circ \alpha_1) \otimes (\beta_2 \circ \beta_1) \).

At this point, we have the basic formal properties to work with tensor products, but we have almost no ability to compute with them. For example, we don’t even know a basis for \( k^m \otimes k^n \)! We turn to this issue next.

\subsection*{D.5} Let \( I \) be a set of vectors spanning \( V \) and let \( J \) be a set of vectors spanning \( W \). Show that the tensor products \( v \otimes w \), for \( v \in I \) and \( w \in J \), span \( V \otimes W \).

\subsection*{D.6} Let \( U \) be a vector space and let \( I \) be a linearly independent subset of \( U \). Prove that there is a basis \( B \) of \( U \) containing \( I \). (This will require Zorn’s Lemma.)

\subsection*{D.7} Let \( U \) be a vector space, let \( I \) be a linearly independent subset of \( U \), and let \( u \in I \). Show that there is a linear form \( \alpha : U \to k \) such that \( \alpha(u) = 1 \) and \( \alpha(u') = 0 \) for \( u' \in I \setminus \{u\} \).

\subsection*{D.8} Let \( I \) be a linearly independent subset of \( V \) and let \( J \) be a linearly independent subset of \( W \). Show that the tensor products \( v \otimes w \), for \( v \in I \) and \( w \in J \), are linearly independent in \( V \otimes W \).

\subsection*{D.9} Let \( I \) be a basis of \( V \) and let \( J \) be a basis of \( W \). Show that the tensor products \( v \otimes w \), for \( v \in I \) and \( w \in J \), form a basis of \( V \otimes W \).

That was a lot of abstraction, so let’s do something concrete.

\subsection*{D.10} Let \( \alpha \) and \( \beta \) be the linear maps \( \mathbb{R}^2 \to \mathbb{R}^2 \) given by the matrices
\[
\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \quad \text{and} \quad \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}.
\]
Choose a basis for \( \mathbb{R}^2 \otimes \mathbb{R}^2 \) and write down the matrix of \( \alpha \otimes \beta \).

“After a few months, though, I realized something. I hadn’t gotten any better at understanding tensor products, but I was
getting used to not understanding them. It was pretty amazing. I no longer felt anguished when tensor products came up; I
was instead almost amused by their cunning ways.” — ibid.











\chapter{Tensor Algebras, Symmetric and Exterior Algebras}

Let \( k \) be a field and let \( V \) be a vector space over \( k \). There is a natural isomorphism \( (V \otimes V) \otimes V \cong V \otimes (V \otimes V) \), and similarly for higher tensor powers. We therefore write \( V^{\otimes n} \) for the \( n \)-fold tensor product of \( V \) with itself and write elements of \( V^{\otimes n} \) as
\[
\sum c_{j_1 j_2 \cdots j_n} v_{j_1} \otimes v_{j_2} \otimes \cdots \otimes v_{j_n}
\]
without parentheses. We define \( V^{\otimes 0} \) to be \( k \).

\begin{definition}
We define the \textit{tensor algebra} \( T(V) \) to be \( \bigoplus_{d=0}^{\infty} V^{\otimes d} \).
\end{definition}

\subsection*{E.1} Show that \( T(V) \) has a unique ring structure where the product of \( \sigma \in V^{\otimes s} \) and \( \tau \in V^{\otimes t} \) is \( \sigma \otimes \tau \in V^{\otimes (s+t)} \).

\subsection*{E.2} Let \( L : V \to W \) be a linear map. Show that there is a unique map of rings \( T(L) : T(V) \to T(W) \) with \( T(L)(v) = L(v) \) for \( v \in V \).

\begin{definition}
We define the \textit{symmetric algebra} \( \operatorname{Sym}^\bullet(V) \) to be the quotient of \( T(V) \) by the two-sided ideal generated by all tensors of the form \( v \otimes w - w \otimes v \).
\end{definition}

\subsection*{E.3} Show that \( \operatorname{Sym}^\bullet(V) \) is a commutative ring.

\subsection*{E.4} Show that \( \operatorname{Sym}^\bullet(V) \) breaks up as a direct sum \( \bigoplus_{d=0}^{\infty} \operatorname{Sym}^d(V) \), where \( \operatorname{Sym}^d(V) \) is a quotient of \( V^{\otimes d} \).

\subsection*{E.5} Let \( x_1, x_2, \dots, x_n \) be a basis of \( V \). Show that \( \{ x_{i_1} x_{i_2} \cdots x_{i_d} : 1 \leq i_1 \leq i_2 \leq \cdots \leq i_d \leq n \} \) is a basis of \( \operatorname{Sym}^d(V) \). Show that \( \operatorname{Sym}^\bullet(V) \cong k[x_1, \dots, x_n] \).

\begin{definition}
We define the \textit{exterior algebra} \( \wedge^\bullet(V) \) to be the quotient of \( T(V) \) by the two-sided ideal generated by \( v \otimes v \) for all \( v \in V \). The multiplication in \( \wedge^\bullet(V) \) is generally denoted \( \wedge \).
\end{definition}

\subsection*{E.6} Show that, for \( v \) and \( w \in V \), we have \( v \wedge w = -w \wedge v \).

\subsection*{E.7} Show that \( \wedge^\bullet(V) \) breaks up as a direct sum \( \bigoplus_{d=0}^{\infty} \wedge^d(V) \), where \( \wedge^d(V) \) is a quotient of \( V^{\otimes d} \).

\subsection*{E.8} Let \( e_1, e_2, \dots, e_n \) be a basis of \( V \). Show that \( \{ e_{i_1} \wedge e_{i_2} \wedge \cdots \wedge e_{i_d} : 1 \leq i_1 < i_2 < \cdots < i_d \leq n \} \) is a basis of \( \wedge^d(V) \).

\subsection*{E.9} Let \( v_1, v_2, \dots, v_d \in V \). Show that \( v_1 \wedge v_2 \wedge \cdots \wedge v_d = 0 \) if and only if \( v_1, v_2, \dots, v_d \) are linearly dependent.

\section{Ring Maps for Symmetric and Exterior Algebras}

\subsection*{E.10} Show that there are unique ring maps \( \operatorname{Sym}^\bullet(L) : \operatorname{Sym}^\bullet(V) \to \operatorname{Sym}^\bullet(W) \) and \( \wedge^\bullet(L) : \wedge^\bullet(V) \to \wedge^\bullet(W) \) with \( \operatorname{Sym}^\bullet(L)(v) = L(v) \) and \( \wedge^\bullet(L)(v) = L(v) \) for \( v \in V \).

\subsection*{E.11} Let \( L : k^3 \to k^3 \) be given by the matrix
\[
\begin{bmatrix} r & s & t \\ u & v & w \\ x & y & z \end{bmatrix}.
\]
Compute the matrix of \( \wedge^2(L) : \wedge^2(k^3) \to \wedge^2(k^3) \).

\subsection*{E.12} Let \( L : k^2 \to k^2 \) be given by the matrix \( \begin{bmatrix} p & q \\ r & s \end{bmatrix} \). Compute the matrix of \( \operatorname{Sym}^2(L) : \operatorname{Sym}^2(k^2) \to \operatorname{Sym}^2(k^2) \).

\subsection*{E.13} Show that \( \wedge^d(L \circ M) = \wedge^d(L) \circ \wedge^d(M) \) and \( \operatorname{Sym}^d(L \circ M) = \operatorname{Sym}^d(L) \circ \operatorname{Sym}^d(M) \).

\section{The Cauchy-Binet Formula}

Given an \( m \times n \) matrix \( X \) with entries in \( k \), and subsets \( I \subseteq \{1, 2, \dots, m\} \) and \( J \subseteq \{1, 2, \dots, n\} \) of the same size, define \( \Delta_{IJ}(X) \) to be the determinant of the square submatrix of \( X \) using rows \( I \) and columns \( J \).

\subsection*{E.14} Prove the Cauchy-Binet formula: Let \( X \) and \( Y \) be \( a \times b \) and \( b \times c \) matrices with entries in \( k \), and let \( I \) and \( K \) be subsets of \( \{1, 2, \dots, a\} \) and \( \{1, 2, \dots, c\} \) with \( |I| = |J| = q \). Then
\[
\Delta_{IK}(XY) = \sum_{J \subseteq \{1, 2, \dots, b\}, |J| = q} \Delta_{IJ}(X) \Delta_{JK}(Y).
\]













\chapter{Bilinear Forms}

Suppose \( k \) is a field and \( V \) is a \( k \)-vector space.

\begin{definition}
A \( k \)-bilinear form on \( V \) is a bilinear pairing \( B : V \times V \to k \). A \( k \)-bilinear form \( B \) is said to be
\begin{itemize}
    \item \textit{symmetric} if \( B(x, y) = B(y, x) \) for all \( x, y \in V \),
    \item \textit{alternating} if \( B(u, u) = 0 \) for all \( u \in V \),
    \item \textit{skew-symmetric} (or anti-symmetric) if \( B(s, t) = -B(t, s) \) for all \( s, t \in V \).
\end{itemize}
\end{definition}

\subsection*{F.1} Show that every alternating form is skew-symmetric. (Hint for this problem and the next two: Think about \( B(v + w, v + w) \).)

\subsection*{F.2} Show that, if the characteristic of \( k \) is not 2, then every skew-symmetric form is alternating.

\subsection*{F.3} Show that, if the characteristic of \( k \) is not 2 and \( B \) is a symmetric bilinear form with \( B(v, v) = 0 \) for all \( v \in V \), then \( B(v, w) = 0 \) for all \( v, w \in V \).

Let \( v_1, v_2, \dots, v_n \) be a basis of \( V \) and let \( G \) be the \( n \times n \) matrix with entries \( G_{ij} = B(v_i, v_j) \). We call \( G \) the \textit{Gram matrix}.

\subsection*{F.4} In the basis \( v_1, \dots, v_n \), verify the formula \( B(\vec{x}, \vec{y}) = \vec{x}^T G \vec{y} \). 
\begin{itemize}
    \item Under what conditions on \( G \) will \( B \) be symmetric?
    \item Under what conditions on \( G \) will \( B \) be alternating?
    \item Under what conditions on \( G \) will \( B \) be skew-symmetric?
\end{itemize}

\subsection*{F.5} Let \( w_1, w_2, \dots, w_n \) be a second basis of \( V \), with \( w_j = \sum S_{ij} v_i \). Let \( H \) be the Gram matrix with entries \( H_{ij} = B(w_i, w_j) \). Give a formula for \( H \) in terms of \( S \) and \( G \).

\begin{definition}
A bilinear form \( B \) on \( V \) is called \textit{nondegenerate} if, for all \( v \in V \), there exists \( w \in V \) with \( B(v, w) \neq 0 \).
\end{definition}

\subsection*{F.6} Let \( V \) be a finite-dimensional vector space. Show that \( B \) is nondegenerate if and only if the Gram matrix of \( B \) is invertible.

\subsection*{F.7} Let \( V \) be a finite-dimensional vector space, let \( B \) be a bilinear form on \( V \), and let \( L \) be a subspace of \( V \) such that the restriction of \( B \) to \( L \) is nondegenerate. Define \( L^\perp = \{v \in V : B(u, v) = 0 \ \forall u \in L\} \). Show that \( V = L \oplus L^\perp \).

\begin{remark}
If we are dealing with a general form, we should define both \( L^\perp = \{v \in V : B(u, v) = 0 \ \forall u \in L\} \) and \( {}^\perp L = \{v \in V : B(v, u) = 0 \ \forall u \in L\} \). Then we have both \( V = L \oplus L^\perp \) and \( V = L \oplus {}^\perp L \). However, for symmetric and skew-symmetric forms, we have \( L^\perp = {}^\perp L \), so we do not need to make this distinction.
\end{remark}

\chapter{Symmetric Bilinear Forms}

Let \( k \) be a field, let \( V \) be a finite-dimensional vector space over \( k \), and let \( B : V \times V \to k \) be a \( k \)-bilinear pairing. Given a basis \( v_1, v_2, \dots, v_n \) of \( V \), we encode \( B \) in a Gram matrix \( G \) with \( G_{ij} = B(v_i, v_j) \). The form \( B \) is symmetric if and only if \( G \) is symmetric. Changing bases of \( V \) modifies the Gram matrix by \( G \to S G S^T \) for invertible \( S \). It is natural to ask how simple we can make the matrix \( G \) through such transformations.

To simplify our results, assume that \( k \) does not have characteristic 2.

\subsection*{G.1} Suppose that \( B \) is a symmetric bilinear form. Show that there is a basis of \( V \) for which the Gram matrix of \( B \) is diagonal. (Hint: If \( B \neq 0 \), use Problem F.3 to find a vector \( v \) with \( B(v, v) \neq 0 \), then consider the decomposition \( V = k v \oplus (k v)^\perp \).)

\subsection*{G.2} Let \( G \) be a symmetric matrix with entries in \( k \). Show that there exists an invertible matrix \( S \) and a diagonal matrix \( D \) such that \( G = S D S^T \).

\subsection*{G.3} Let \( k = \mathbb{Q} \). Perform the diagonalization procedure in the previous problems for
\begin{enumerate}
    \item \( G = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \),
    \item \( G = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix} \).
\end{enumerate}

This raises the question of when two diagonal matrices \( \operatorname{diag}(\alpha_1, \alpha_2, \dots, \alpha_n) \) and \( \operatorname{diag}(\beta_1, \beta_2, \dots, \beta_n) \) represent equivalent bilinear forms. For a general field, this question is challenging, but we can establish some results.

\subsection*{G.4} Suppose that there exist nonzero scalars \( \gamma_i \) in \( k \) such that \( \alpha_i = \gamma_i^2 \beta_i \). Show that the bilinear forms \( \vec{x}^T \operatorname{diag}(\alpha_1, \alpha_2, \dots, \alpha_n) \vec{y} \) and \( \vec{x}^T \operatorname{diag}(\beta_1, \beta_2, \dots, \beta_n) \vec{y} \) are equivalent.

\subsection*{G.5} Show that the bilinear forms \( B\left(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \begin{bmatrix} y_1 \\ y_2 \end{bmatrix}\right) = x_1 x_2 + y_1 y_2 \) and \( C\left(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \begin{bmatrix} y_1 \\ y_2 \end{bmatrix}\right) = 5 x_1 x_2 + 5 y_1 y_2 \) are related by a change of basis in \( \mathbb{Q}^2 \), even though 5 is not a square in \( \mathbb{Q} \).

\subsection*{G.6} Let \( k = \mathbb{R} \). Show that any bilinear form over \( \mathbb{R} \) can be represented by a diagonal matrix whose entries lie in \( \{-1, 0, 1\} \).








\chapter{Symmetric Bilinear Forms over \( \mathbb{R} \)}

Let \( B \) be a symmetric bilinear form on a vector space \( W \) over \( \mathbb{R} \). We say that \( B \) is
\begin{itemize}
    \item \textit{positive definite} if \( B(w, w) > 0 \) for all nonzero \( w \in W \),
    \item \textit{positive semidefinite} if \( B(w, w) \geq 0 \) for all \( w \in W \),
    \item \textit{negative definite} if \( B(w, w) < 0 \) for all nonzero \( w \in W \),
    \item \textit{negative semidefinite} if \( B(w, w) \leq 0 \) for all \( w \in W \).
\end{itemize}

Recall that we showed in Problem G.6 that a symmetric bilinear form over \( \mathbb{R} \) can always be represented by a diagonal matrix with entries in \( \{-1, 0, 1\} \).

\subsection*{H.1} Let \( B \) be a symmetric bilinear form represented by the diagonal matrix
\[
\operatorname{diag}(\underbrace{1, 1, \dots, 1}_{n_+}, \underbrace{0, 0, \dots, 0}_{n_0}, \underbrace{-1, -1, \dots, -1}_{n_-}).
\]
\begin{enumerate}
    \item Show that \( n_+ \) is the dimension of the largest subspace \( L \) of \( V \) such that \( B \) restricted to \( L \) is positive definite.
    \item Show that \( n_+ + n_0 \) is the dimension of the largest subspace \( L \) of \( V \) such that \( B \) restricted to \( L \) is positive semidefinite.
    \item Show that \( n_- \) is the dimension of the largest subspace \( L \) of \( V \) such that \( B \) restricted to \( L \) is negative definite.
    \item Show that \( n_- + n_0 \) is the dimension of the largest subspace \( L \) of \( V \) such that \( B \) restricted to \( L \) is negative semidefinite.
\end{enumerate}

\subsection*{H.2} Let \( B \) be a symmetric bilinear form. Suppose \( B \) can be represented (in two different bases) by the diagonal matrices
\[
\operatorname{diag}(\underbrace{1, 1, \dots, 1}_{m_+}, \underbrace{0, 0, \dots, 0}_{m_0}, \underbrace{-1, -1, \dots, -1}_{m_-}) \quad \text{and} \quad \operatorname{diag}(\underbrace{1, 1, \dots, 1}_{n_+}, \underbrace{0, 0, \dots, 0}_{n_0}, \underbrace{-1, -1, \dots, -1}_{n_-}).
\]
Show that \( (m_+, m_0, m_-) = (n_+, n_0, n_-) \).

\begin{remark}
The word \textit{signature} refers to something like the triple \( (n_+, n_0, n_-) \). In this course, we adopt the convention that the signature is \( (n_+, n_0, n_-) \). If \( G \) is a symmetric real matrix, we refer to the signature of \( G \) as the signature of the bilinear form \( B(x, y) = x^T G y \).
\end{remark}

\subsection*{H.3} Let \( G \) be a real symmetric \( n \times n \) matrix with signature \( (n_+, n_0, n_-) \).
\begin{enumerate}
    \item If \( n_0 > 0 \), show that \( \det G = 0 \).
    \item If \( n_0 = 0 \), show that \( \det G \) is nonzero with sign \( (-1)^{n_-} \).
\end{enumerate}

\subsection*{H.4} Let \( G \) be a real symmetric \( n \times n \) matrix with signature \( (n_+, n_0, n_-) \). Let \( G' \) be the upper-left symmetric \( (n-1) \times (n-1) \) submatrix of \( G \). Show that the signature of \( G' \) is one of \( (n_+ - 1, n_0 + 1, n_- - 1) \), \( (n_+ - 1, n_0, n_-) \), \( (n_+, n_0, n_- - 1) \), or \( (n_+, n_0 - 1, n_-) \). (Hint: Use Problem H.1.)

\subsection*{H.5} Let \( G \) be a real symmetric matrix, and let \( G_k \) be the \( k \times k \) upper-left submatrix of \( G \). Assume that \( \det G_k \neq 0 \) for \( 1 \leq k \neq n \). Show that the signature of \( G \) is \( (n - q, 0, q) \), where \( q \) is the number of \( k \) for which \( \det G_{k-1} \) and \( \det G_k \) have opposite signs. Here we formally define \( \det G_0 = 1 \).

\subsection*{H.6} (\textbf{Sylvester’s Criterion}) Let \( G \) be a real symmetric matrix and define \( G_k \) as above. Show that \( G \) is positive definite if and only if all the determinants \( \det G_k \) are positive. (In other words, we no longer have to assume \( \det G_k \neq 0 \).)


\end{document}
